\chapter{不確かさを考慮した行動決定}

本章では、不確かさを考慮した行動決定について述べる。
まず、状態が既知のロボットの最適な行動決定を扱う枠組みである「マルコフ決定過程」について述べる。
その後、ロボットの状態が不確かにしか分からない状況での最適な行動を扱う枠組みである「部分観測マルコフ決定過程」について述べる。
最後に、「部分観測マルコフ決定過程」を近似的に解く手法について、いくつかあるうち、本研究と関連のあるQ-MDP法およびPFC法について述べる。


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ロボットの行動決定}
本節では、ロボットの行動決定について考える。
本論文で扱うような移動ロボットの行動決定は、現在のロボットの位置から目的地までの最短経路を算出して移動する、「経路計画問題」として扱われる。
ダイクストラ法やA*法、人工ポテンシャル法といった多くの手法が提案されており、現在でも研究が行われている。
一般的にロボットの経路計画問題では、ロボットの観測や移動は決定論的なもとのして扱われ、経路の算出が行われる。

しかし、これまで述べたとおり、移動ロボットは多くの不確かさを有している。
移動ロボットに最短の経路を算出して移動するだけでなく、これらの不確かさを考慮した上でさらに知的な動作を行わせる方法について考える必要がある。
たとえば、人間であれば、最短ではあるが危険でゴールに到達できる可能性が低い道よりも、多少遠回りではあるが高確率で安全にゴールできる道を選択するような、
「急がば回れ」が有効な場合も存在する。
あるいは、自身の位置が正しく把握できていない場合に、安全を優先し全く動かないでいるよりも、
多少の危険を冒しとりあえず行動してみてタスク達成を目指す方が有効であるような場合も考えられる。

このような知的な行動を、単純に経路計画問題として考えることは困難である。
そこで本章では、ロボットの移動と経路計画について、
より一般化した枠組みである（有限）マルコフ決定過程および部分観測マルコフ決定過程について述べる。

%%%%%%%%%%%%%%
%%%%ロボットの不確かさは大きく分けて、動作に関するものと観測に関するものの2つに分類できる。


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{マルコフ決定過程} \label{section:mdp}
本節では、移動ロボットの現在の自己位置$\bm{x} \in \mathcal{X}$が既知という前提での行動決定についての定式化を行う。
このような問題は、マルコフ決定過程(Markov decision process, MDP)という枠組みで議論される。
時間やロボットの状態等を連続系ではなく離散系で考える場合は、有限マルコフ決定過程と呼ばれる。
% また、本論文では、とくに断りがない場合MDPは離散系の有限マルコフ決定過程のことを指す。

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{状態と終端状態}
章\ref{chapter:localization}で述べたものと同様に、
状態変数$x,y,\theta$からなる状態空間$\mathcal{X}$を定義する。
ロボットは自身の現在の状態ベクトル$\bm{x} \in \mathcal{X}$が分かっているものとし、観測の不確かさについては考慮しない。

ロボットのタスクには必ず終わりがあるとし、
ロボットの状態が、事前に定めたある状態になったときをタスクの終了とする。
このタスクが終了する状態は、終端状態と呼ばれ、終端状態の集合は$\mathcal{X}_f \subset \mathcal{X}$と表現される。
終端状態は、移動ロボットの経路計画問題における、ゴールや目標地点などのロボットが目指すべき望ましい状態だけでなく、陥りたくない状態も含まれる。

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{行動と状態遷移}
ロボットは有限子の制御指令の中から、一つを選択することで動作する。
MDPではこの制御指令のことを行動と呼ぶ。
ロボットの行動はm種類存在し、行動の集合は、
\begin{equation}
\label{action}
  \mathcal{A} = \{ a_{1}, a_{2}, \ldots , a_{m} \}
\end{equation}
と表現される。

時間は離散的に表現され、タスクの開始からは終わりまでは${0,1,2,\ldots,t_{f}} \equiv T$と定義される。
ロボットが最初に行動を選択する時刻を$t=0$とし、行動が実行されるたびに$t=1,2,\ldots$と次のステップへと進んでいく。
ロボットが終端状態に入りタスクが終了する時刻を$t_{f}$とする。
$t_{f}$は固定ではなく、タスク達成までにかかった時間により異なる。
また、「時刻$t-1$の状態」、「時刻$t$に遷移するために選択された行動」、「時刻$t$の状態」をそれぞれ$\bm{x}_{t-1}, a_{t}, \bm{x}_{t}$と表現する。
しかし、MDPで扱うシステムは時不変であるため、多くの場合$t$の具体的な値は重要でははない。
したがて、今後はこれらをそれぞれ$\bm{x}, a, \bm{x}^{'}$と表記する。

ロボットの状態$\bm{x}$は、ある行動$a$により状態$x^{'}$へと遷移する。
状態の遷移にはノイズが含まれているものとし、状態と行動が同一の$(\bm{x}, a)$であっても、事後状態$\bm{x}^{'}$は各試行ごとに異なる。
ロボットの状態遷移は、マルコフ性を持ち、
\begin{equation}
\label{trans prob}
  \bm{x} \sim p(\bm{x} | \bm{x}_{t-1}, \bm{u}_t) (t=1,2,\ldots,t_{f})
\end{equation}
に従うものとする。
また、ロボットが$\bm{x}$から行動$a$により$\bm{x}^{'}$に遷移する確率を$p^{a}_{\bm{x}\bm{x}^{'}}$と表現する。
この確率$p^{a}_{\bm{x}\bm{x}^{'}}$が、$\bm{x}, a, \bm{x}^{'}$のあらゆる組に対して既知であり、時不変であると仮定する。

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{報酬} \label{subsection:reward}
状態$\bm{x}$で行動$a$を行った場合に、状態が$\bm{x}^{'}$に遷移した際に、その状態遷移に対して報酬を与える。
たとえば、移動ロボットではこの報酬は、行動一回ごとに消費する電力や時間などを設定する。
この報酬は、一つの実数値とし、$r^{a}_{\bm{x}\bm{x}^{'}} \in \mathbb{R}$と表現される。
評価の基準が多次元的の場合も、一つの実数となるように一元化し評価を行う。

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{評価}
ロボットがある状態$\bm{x}$からある終端状態$\bm{x}_{f} \in \mathcal{X}_{f}$に到達するまでの一連の状態遷移に対して評価を与える。
評価は、
\begin{equation}
\label{evaluation}
  J( \bm{x}_{0:t_{f}}, a_{1:t_{f}} ) = \sum^{t_{f}}_{t=1} r^{a}_{\bm{x}\bm{x}^{'}} + V(\bm{x}_{t_{f}})
\end{equation}
で定義される評価値の大きさで行う。
$r^{a}_{\bm{x}\bm{x}^{'}}$は、項\ref{subsection:reward}で述べた報酬である。
$V(\bm{x}_{t_{f}})$は終端状態の価値とし、事前に目的に応じて与えられるものとする。
ロボットが目指すゴールや目標地点とする場合、$0$を設定する。
逆に、ロボットがその状態になった場合タスクが失敗として終了するような、絶対に陥ってほしくない状態として設定する場合、非常に小さい値を設定する。

本論文では、この評価値$J$を最大化する一連の状態遷移を、最適な制御とする。
状態遷移にはノイズが伴うため、同じ姿勢$\bm{x}_{0}$から常に最適となるような行動をとったとしても、試行ごとに評価値$J$は異なる。
そのため、ある状態から終端状態までの一連の状態遷移に対する評価$J$は、期待値として考える。

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{最適方策}
ある状態における、Jの期待値を最大化するための行動を与える関数
\begin{equation}
\label{policy}
  \Pi : \mathcal{X} \rightarrow \mathcal{A}
\end{equation}
を定義する。
この関数$\Pi$は最適方策と呼ばれる。
最適方策$\Pi$が決まると、任意の状態$\bm{x}$でロボットが取るべき行動は、
\begin{equation}
\label{oprimal action}
  a = \Pi(\bm{x}) \;\;\;\; (\forall \bm{x} \in \mathcal{X})
\end{equation}
と自動的に決まる。

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{最適価値関数}
また、ある状態に対して評価値の期待値$J$を与える関数
\begin{equation}
\label{value function}
  V : \mathcal{X} \rightarrow \mathbb{R}
\end{equation}
は、最適価値関数と呼ばれる。
$V(\bm{x})$は、$\bm{x}$が初期の状態でも、タスクにおける途中の状態でも変わらない。
終端状態の価値$V(\bm{x}_{t_{f}})$は、この最適価値関数に含まれる。


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{部分観測マルコフ決定過程}
本節では、移動ロボットの現在の状態が不確かな中での行動決定について述べる。
このような問題は、部分観測マルコフ決定過程(partially observable Markov decision process, POMDP)という枠組みで研究が行われている。

POMDPがMDPと異なる点は、ロボットが真の状態が分かっていないという点にある。
ロボットはMDP同様、評価値$J$を最小化するように終端状態にたどり着くことを目的とする。
しかし、ロボットの姿勢$\bm{x}$が未知であるため、最適方策$\Pi(\bm{x})$を使用することができない。
したがって、$\bm{x}$の代わりにその場で得られる情報をもとに行動を決定する必要がある。
ロボットがこれまで行ってきた行動の履歴$a_{1:t}$、それまでに得た観測の情報$z_{1:t}$、およびそれまでに得た報酬の履歴$r_{1:t}$から決定される方策は
\begin{equation}
\label{policy pomdp}
  a_{t+1} = \Pi_{\rm POMDP} (a_{1:t}, \bm{z}_{1:t}, r_{1:t})
\end{equation}
と定義される。

ロボットが行動を決定するとき、その時点での状態推定の不確かさを考慮したい場合、関数
\begin{equation}
\label{policy belief}
  a_{t+1} = \Pi_{\rm b} (b_{t})
\end{equation}
を考えればよいことになる。
この関数は、ロボットの姿勢を状態とするのではなく、ロボットの信念分布自体をひとつの状態としてみなし、
とるべき行動をロボットの現在の信念分布から決定する。
状態としてみなされる信念分布は、信念状態と呼ばれる。

このように信念の分布をひとつの状態とみなすことで、POMDPの問題をMDPの問題として考えることができる。
信念状態を用いることでPOMDPをMDPとして考える方法は、belief MDPと呼ばれる\cite{kaelbling1998}。
しかし、信念状態の数は非常に膨大であり、最適方策を得ることは通常不可能とされている。


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Q-MDP}
本節では、$Q_{\rm MDP}$ value methodについて述べる。
この手法は、Littmanらにより\cite{littman1995}で提案され、Uedaらによって\cite{}でロボットに適用された。

Q-MDP法は、観測の不確かさを考慮せずにロボットの行動を


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Probabilistic Flow Control}
