\chapter{シミュレーションによる評価} \label{chapter:evaluate}
本章では、提案した手法の評価を行う。
\ref{section:評価方法}節では、評価方法について述べる。
\ref{section:実験条件}節では、シミュレーション実験の条件について述べる。
\ref{section:結果}節では、シミュレーション実験の結果と考察を述べる。

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{評価方法} \label{section:評価方法}
本節では、評価方法について述べる。
まず、評価を行うにあたって、本手法により目指している動作について述べる。
そして、その目指している動作を達成しているかを評価するときの基準について述べる。

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{目指している動作}
本論文において、移動ロボットに行わせたい動作は、状態推定の不確かさを考慮した障害物回避の動作である。
通常行われるような、最も確率の高い姿勢を真の姿勢と仮定して行われる行動計画では、確率的な推定の情報が行動に生かされない。
そこで、信念分布内のどこかにロボットが存在する可能性を考慮し、
信念分布を近似するパーティクル全体が、障害物内に入らないような行動を行わせることを目指している。

また、PFC法のゴール探索動作が行われることも求められる。
分布全体を徐々にゴールになぞるように流し込み、分布内に存在するロボットがいずれゴールへと到達する。
これにより、信念分布がゴール範囲よりも大きい場合でも、ロボットがゴールすることを可能にする。

信念分布が大きいときに、この2つの動作をそれぞれ状況にあわせて行うことを目指す。
周囲に障害物があるときには、分布全体が障害物を避けるような動作を行わせる。
そして、ゴール周辺では、停滞することなくゴールへと到達するするために、分布をゴールへと流し込む探索動作を行わせる。

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{評価方法}
１回のナビゲーションタスクにおいて、障害物の回避とゴールの探索動作がそれぞれ行えているかを評価する。
タスクは、ロボットがゴールへと到達したときに成功とみなす。
逆に、ロボットが障害物内に侵入した時点で、そのタスクは失敗とする。
ロボットがゴールへ到達するのに300秒以上かかった場合も、同様に失敗とする。

ロボットは初期姿勢の配置と動作モデルは、それぞれノイズを有している。
ロボットの初期姿勢は、正規分布に従うばらつきを有しており、
パーティクルの初期分布も、同様の正規分布に従い配置されるものとする。

また、ロボットの状態推定の不確かさが大きい状況を維持するために、
ロボットが観測により得られる情報が非常に制限されているものとする。
自身の姿勢推定に用いることができる情報は、ゴールしているか否かの情報のみである。
これは、PFC法による行動を行うために必要となる観測情報である。
そのほかの距離センサやランドマークによる観測は行えないものとする。

そのために、ロボットがナビゲーションにおいて、状態推定を考慮した障害物回避とゴールの探索を行えているかを、
\begin{itemize}
  \item タスクの成功率
  \item タスク成功時の平均時間
  \item 障害物内を移動したパーティクル数×時間 の平均
\end{itemize}
の3つを基準に評価する。
この基準をもとに、提案手法を、通常のPFC法、Q-MDP法、ロボットの真の姿勢による最適方策、
そしてパーティクルの平均姿勢による最適方策の4つと比較する。
各手法それぞれの試行を100回行うことで評価する。


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{シミュレーション実験の条件} \label{section:実験条件}
本節では、シミュレーションによる実験の条件について述べる。
まず、評価を行うための環境について述べる。
そして、ロボットの初期姿勢や状態遷移確率、行動の種類について述べ、また、MCLやその他パラメータについて述べる。

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{環境}
二次元のシミュレータを用いて、評価を行う。
評価は、図\ref{fig:environment}に示すように、一つの障害物を有する、幅が$10[\si{m}]$の正方形の空間で行う。
灰色で示された領域は、障害物を意味する。
また、環境の中央を原点とする$X-Y$座標系$\Sigma$を定義する。
$X$軸と$Y$軸はそれぞれ環境の縁と並行に設定されている。
ゴールは$(0, 1)$の位置を中心とした、半径$0.15[\si{m}]$の範囲内とする。
\begin{figure}[H]
  \begin{center}
    \includegraphics[width=15cm, ]{environment.pdf}
    \caption{Environment map with one obstacle}
    \label{fig:environment}
  \end{center}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{タスク内容の成功条件}
ロボットに、ゴールまでのナビゲーションタスクを行わせる。
図\ref{fig:task}に示す実線矢印のように、障害物を回避しゴールへ入ることができたらタスク成功とする。
逆に、破線矢印のようにロボットが障害物に衝突した場合、タスク失敗とする。
また、$300[\si{s}]$の間ゴールへ入ることができなかった場合も、同様にタスク失敗とする。

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=15cm, ]{task.pdf}
    \caption{Task success and failure examples}
    \label{fig:task}
  \end{center}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{ロボットの行動と初期姿勢}
ロボットの行動は、図\ref{fig:ロボットの行動の種類}に示すような、
$\mathcal{A} = { fw, ccw, cw }$の3つから選択されるものとする。

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=10cm, ]{ロボットの行動の種類.pdf}
    \caption{Types of robot action}
    \label{fig:ロボットの行動の種類}
  \end{center}
\end{figure}

行動はそれぞれ、
\begin{itemize}
  \item $fw$: $(0.2 + 0.01\sigma) \si{[m/s]}$ の前進
  \item $ccw$: $(1 + 0.01\sigma) \si{[rad/s]}$ のその場旋回
  \item $cw$: $(-1 + 0.01\sigma) \si{[rad/s]}$ のその場旋回
\end{itemize}
を意味する。
ここで、$\sigma$は正規分布$\mathcal{N}(0, 1)$から生成されるノイズであり、$\sigma \sim \mathcal{{N}}(0, 1)$のように選択される。
ロボットは、この運動モデルを知っていると仮定する。

ロボットの初期姿勢は、
\begin{equation}
\label{robot initial pose}
  (x, y, \theta) = (-3 + 0.3\sigma, -3 + 0.3\sigma, 0.03\sigma)
\end{equation}
のように配置され、各試行ごとに正規分布に従いばらつく。
% 初期配置の例を図に示す。

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{価値関数の計算}
価値関数を計算する状態空間$\mathcal{X}$は、二次元の位置$(x, y)$と方向$\theta$の3状態からなる。
計算のための離散化は、位置についての離散状態の幅を$0.05\si{[m]}$とし、方向については$10\si{[deg]}$とする。
図\ref{fig:environment}に示した縦横の幅が$10\si{[m]}$の空間を離散化すると、離散状態は$200 \times 200 \times 36$となる。
この、合計$1,440,000$離散状態の価値関数を事前に計算する。
また、ゴール内に含まれている状態$s$を終端状態$\mathcal{S}_f$とする。

状態遷移の報酬について設定する。
このタスクでは、ゴールまで早く到達することと、障害物を避ける移動が良い移動となるため、
経過時間と障害物への侵入に負の報酬（ペナルティー、コスト）を与えることにする。
まず、時間に関する報酬を、
\begin{equation}
\label{robot initial pose}
  {r_{\rm time}}^{a}_{\bm{x}\bm{x}^{\prime}} = -\Delta t
\end{equation}
と設定する。ここで、$\Delta t$は、$1$ステップの離散時間を意味し、ロボットが一回の行動で移動する時間とする。
今回は、この$\Delta t$は$0.1[s]$とする。
次に、障害物内での移動に関する報酬を
\begin{equation}
\label{robot initial pose}
  {r_{\rm obstacle}}^{a}_{\bm{x}\bm{x}^{\prime}} = -c\Delta t
\end{equation}
と設定する。状態遷移先が障害物だった場合、負の報酬として移動時間$\Delta t$の$c$倍を与える。
MDPでは、報酬は一元化する必要があるため、この2つの報酬を合わせて、
\begin{equation}
\label{robot initial pose}
\begin{split}
  r^{a}_{\bm{x}\bm{x}^{\prime}} &=
  {r_{\rm time}}^{a}_{\bm{x}\bm{x}^{\prime}} +
  c{r_{\rm obstacle}}^{a}_{\bm{x}\bm{x}^{\prime}} \\
  &= -\Delta t  -c\Delta t
\end{split}
\end{equation}
と設定する。
$c$は定数であり、障害物へ入ることのペナルティー度合いを表す。
今回のタスクでは、障害物に入ることが許されないため、$c$の値を大きく$100$と設定する。

価値関数の計算は、
\url{https://github.com/ryuichiueda/simple_value_iteration_ros.git}
のコードを利用して行った。
計算には、インテル Core i7 8550U/1.8GHz/4コアのCPUを搭載したコンピュータを用いて、32分58秒かかった。
計算結果の一部を示している図\ref{fig:価値関数計算結果}は、
方向を$0 \leq \theta < 10 \si{[deg]}$で固定したときの、各位置での価値を表している。
白が最も価値が高く、色が濃くなるほど価値が低いことを意味する。
ゴールは価値が$0$であり、ゴール遠方や障害物内は低い価値となっていることが分かる。

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=15cm, ]{価値関数計算結果.pdf}
    \caption{Calculation result of state value function $(0 \leq \theta < 10 \si{[deg]})$}
    \label{fig:価値関数計算結果}
  \end{center}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{MCLの設定}
MCLのパーティクル数$N$は、$500$で固定とする。
パーティクルの初期姿勢は、ロボットの初期配置のばらつきと同様に分布させる。
図\ref{fig:パーティクルの初期分布}に、タスク開始時のロボットの姿勢とパーティクルの分布の一例を示す。
パーティクルは正規分布に従いばらまかれ、ロボットも同様の正規分布に従い初期配置が行われる。

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=15cm, ]{パーティクルの初期分布.pdf}
    \caption{Example of initial distribution of particles and initial pose of robot}
    \label{fig:パーティクルの初期分布}
  \end{center}
\end{figure}

予測ステップにおけるパーティクルの状態遷移は、ロボットの運動モデルと同様とする。
ロボットは、タスクが終了したかどうかの情報を得ることができる。
この情報を計測による更新ステップで、パーティクルの重み$w^{(i)}$に反映する。
尤度関数は
\begin{equation}
  q({\rm not \  finished} | \bm{x}^{(i)}) =
  \left\{
    \begin{array}{l}
      10^{-10} \ \ (\bm{x}^{(i)} \in \mathcal{X}_{f}) \\
      1 \quad \quad \ \ ({\rm otherwise})
    \end{array}
  \right.
\end{equation}
とし、タスク実行中にゴール内へと侵入したパーティクルの重みを非常に小さくする。

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{PFC法の設定}
本実験では、PFC法においてゴールに近いパーティクルが行動決定に与える影響度を表す$m$は、$2$とする。
$m$の値を変えたときの、タスクの達成率と達成までの時間の関係は、\cite{ueda2018searching}において明らかにされている。
この研究では、触覚センサのみを有する簡易マニピュレータによる、把持物体の探索動作において検証されている。
移動ロボットのタスクにおける$m$とタスクの達成率、および達成までの時間については、まだ検証されていない。

本論文において新たに導入した、障害物に侵入しそうなパーティクルが、行動決定与える影響を変更する変数$m^{(i)}_{\rm avoid}$を設定する。
最大値と最小値については、それぞれ$m_{\rm avoid\_min} = 1$と$m_{\rm avoid\_max} = 3$とする。

また、本評価では、ロボットがデッドロックすることを防ぐための単純な処理を追加する。
ロボットは、自身が左右の旋回行動$ccw, cw$を交互に行った場合、前進する行動$fw$を行う。


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{結果と考察} \label{section:結果}
本節では、シミュレーションによる評価実験の結果と考察を述べる。
まず、それぞれの手法を定量的に比較する。
そして、それぞれの手法で生成された挙動について確認する。

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{他手法との比較}
本項では、提案手法と他手法の比較を行う。
\ref{section:実験条件}節で述べた条件、タスクのもと、提案手法を4つの手法と比較した。
本論文で提案した障害物回避行動を取り入れたPFC法を、
回避動作を入れていない通常のPFC法と、Q-MDP法($m=0$のPFC法)と比較する。
さらに、参考として、
\begin{itemize}
  \item ロボットの真の姿勢$\V{x}^{*}$を使用した最適方策
  \item パーティクルの平均姿勢$\bar{\V{x}}$を使用した最適方策
\end{itemize}
の2つを加えた、5つの手法で比較する。

結果を表\ref{table:comparison}に示す。
各列の数値は、左から順に
\begin{enumerate}
  \item タスクの成功率
  \item タスク成功時の平均時間
  \item タスク成功時における障害物内を移動したパーティクル数 $\times$ 時間の平均時間
\end{enumerate}
を表している。

\begin{table}[H]
  \begin{center}
\label{table:comparison}
\caption{Comparison of PFC with avoidance, nomal PFC, Q-MDP, and some other simulations}
\vspace{0.5cm}
\begin{tabular}{@{}lccc@{}}
\toprule
Methods &
  \begin{tabular}[c]{@{}c@{}}Successful\\ trials\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Avg. of time\\ in successful trials\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Avg. of the total time\\ of particles inside obstacle\\ in successful trials\end{tabular} \\ \midrule
PFC with avoidance    & 99 $\%$ & 72.3 $[\si{s}]$ & 0.0    $[\si{s}]$ \\
PFC                   & 51  $\%$ & 63.0 $[\si{s}]$ & 2587.1 $[\si{s}]$ \\
Q-MDP                 & 40  $\%$ & 65.0 $[\si{s}]$ & 22.6   $[\si{s}]$ \\
Decision based on $\bar{\V{x}}$  & 20  $\%$ & 38.9 $[\si{s}]$ & 898.9  $[\si{s}]$ \\
Decision based on $\V{x}^{*}$   & 100 $\%$ & 39.6 $[\si{s}]$ & 994.5  $[\si{s}]$ \\ \bottomrule
\end{tabular}
  \end{center}
\end{table}

当然、ロボットの真の姿勢$\V{x}^{*}$を行動決定に使用した場合、タスクの成功率は$100\%$となっているが、
パーティクルの平均姿勢$\bar{\V{x}}$を利用した行動決定では、タスクの成功率は$20\%$と低い値になっていることが分かる。
2つの手法において、タスク成功時の平均時間はほぼ同様の時間である。
これは、$\bar{\V{x}}$を使用してタスクが成功したときには、$\V{x}^{*}$が$\bar{\V{x}}$の付近の少し右下に存在するからである。
大きく右下方向に離れている場合は、障害物に衝突することはなくなるが、ゴール周辺に到達しても実際にゴールすることがないため、タスク失敗となる。

本論文で提案した手法は、本評価実験においては通常PFC法に比べてタスク達成率が$99\%$に大幅に向上していることが分かる。
パーティクルが障害物の中を移動した時間が$0[\si{s}]$であるとおり、分布全体が障害物を確実に回避している。
通常のPFC法では、状態価値が低い障害物内のパーティクルが行動決定に大きな影響を与えるため、
多くのパーティクルが長時間に渡って障害物内を移動している。
Q-MDP法では、状態価値による影響の違いがないため、パーティクルの障害物内移動がPFC法よりも少ない。
今回の評価では障害物内の報酬${r_{\rm obstacle}}^{a}_{\bm{x}\bm{x}^{\prime}}$をかなり小さく設定しているため、
Q-MDP法でも簡単にパーティクルが障害物内に入っていくことは少ない。
しかし、全体のパーティクル数に対して僅かな数であれば、侵入していくことがあるため、比較的少ないが$0$にはならない。

提案手法において失敗した$1\%$は、初期姿勢のズレが平均値から大きく離れていたことや、運動モデルのばらつきにより、
$\V{x}^{*}$が分布から外れてしまった場合である。
今回の実験条件では、ロボットの観測が制限されていたため、このような自体も起こり得る。
また、ロボットに搭載したセンサにより、自己位置推定に役立つ観測を行える場合であっても、分布が$\V{x}^{*}$と乖離する場合がある。
このような状態を「誘拐状態」と言い、再度パーティクルを$\V{x}^{*}$の周辺に分布させることは困難とされている。
% これは、「ロボット誘拐問題」と呼ばれ、復帰方法についてもいくつか研究が行われている\cite{lenser2000etal, 上田etal2005b}。
これは、「ロボット誘拐問題」と呼ばれ、復帰方法についてもいくつか研究が行われている\cite{lenser2000etal}。
本評価においては、$\V{x}^{*}$が分布ないに存在していることを前提とし、誘拐状態からの回復アルゴリズムを実装していないため、
タスクの失敗へとつながった。

提案手法ではタスク成功率から、障害物回避後のゴール探索動作についても行えていることが分かる。
今回の実験条件では、ロボットの運動モデルのノイズを小さく設定しているため、移動による分布の広がりが小さい。
そのため、分布をゴールに流しこむ動作にかかる時間があまり長くならずに済んでいると考えられる。
Q-MDP法では、パーティクルが障害物内を移動している時間が比較的短いにも関わらず、タスク達成率は半分以下と低くなっている。
これは、障害物を回避することができても、ゴール付近で探索動作を行うことができず、打ち切りの時間まで停滞しタスク失敗となるためである。

タスク成功時の平均時間については、本論文において提案した手法が、通常のPFC法やQ-MDP法と比べて一番長いことが分かる。
パーティクルの分布を行動決定に利用する3つの手法の中では、通常のPFC法のタスク達成時間が最も短い。
これは、通常のPFC法は、障害物内のパーティクルの行動が優先されるため、3つの手法の中で最も分布がインコース側を移動するためである。
ロボットが分布のアウトコース側に存在し、障害物を回避することができたとき、最も早くゴールへと到達することができるのだと考えられる。
Q-MDP法は、通常のPFC法よりも分布が障害物内を避けるようにアウトコース側を移動するため、成功時の時間が少し短いと考えられる。
提案手法では、パーティクルの分布全体が確実に障害物を避けるようにゴールへと向かうため、タスク達成までの時間は一番長くなってる。


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{生成された挙動の確認}
本項では、各手法により生成された挙動を確認する。
動作の様子を確認し、挙動の特性について考えていく。

まず、表\ref{table:comparison}における下2つの挙動について確認する。
ロボットの真の姿勢$\V{x}^{*}$を使用した行動決定は、すべての試行において成功した。
ロボットは、図\ref{fig:true pose}に示すように、障害物回避し、かつ最短の経路を通ってゴールへと向かう。
軌跡が角張っているのは、状態空間を離散化したときの粗さが影響していると考えられる。
今回は$\theta$を$10[\si{deg}]$ごとに離散化したが、もっと細かく離散化することで、軌跡はなめらかなものになると予想される。

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=15cm, ]{true_pose.pdf}
    \caption{Robot behavior by true pose}
    \label{fig:true pose}
  \end{center}
\end{figure}

パーティクルの平均姿勢$\bar{\V{x}}$を行動決定に利用した場合、図\ref{fig:average pose}に示すような挙動となる。
パーティクルの初期姿勢は、正規分布に従うノイズによるばらつきがあるため、$\bar{\V{x}}$は分布のほぼ中心に位置する。
図の上部の試行では、ロボットの真の姿勢$\V{x}^{*}$は分布の右下にあるため、障害物付近を通る際アウトコース側を移動する。
したがって、障害物に突入することなくゴールへと向かっている。
一方で、図の下部の試行では、$\V{x}^{*}$がインコース側を移動しているため、障害物に衝突してしまっているのが分かる。

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=15cm, ]{average_pose.pdf}
    \caption{Robot behavior by particles average pose}
    \label{fig:average pose}
  \end{center}
\end{figure}

続いて、提案手法による動作について確認する。
図\ref{fig:avoid pfc good}に、提案手法による試行の一例を示す。
提案手法では、パーティクル全体が障害物を避けるように移動している。
そして、ゴール周辺へとたどり着いた後、ゴール探索動作を行いタスク成功となっている。
$t=12.7[\si{s}]$でひとつのパーティクルが障害物に入りそうになった際、$m^{(i)}_{\rm avoid}$の値が最大値となる。
ロボットはそのパーティクルの行動を最優先するように右旋回ののち前進し、パーティクルが障害物に侵入するのを防ぐ動作を行った。

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=15cm, ]{avoid_pfc_good.pdf}
    \caption{Robot behavior by PFC method with avoidance}
    \label{fig:avoid pfc good}
  \end{center}
\end{figure}

しかし、提案手法の動作には、無駄な動きの多さが伺える。
図\ref{fig:avoid pfc kakukaku}に示す試行は、ロボットの無駄な動作が分かりやすく現れている。
$t=15.3[\si{s}]$でパーティクルが一つ障害物に侵入しそうになり、$t=17.7[\si{s}]$で回避するように動作する。
その後、再度$t=19.8[\si{s}]$で別のパーティクルが障害物に親友しそうになり、$t=12.7[\si{s}]$で回避動作をする。
このように、パーティクルの分布によっては、ゴールへ向かう動作と回避動作が何度も交互に繰り返され、ギザギザの軌跡を描く。

本手法では、次の動作によるパーティクルの報酬のみから、$m^{(i)}_{\rm avoid}$を決定しているため、
このようにその場しのぎのような動作になると考えられる。
これを防ぐためには、数ステップ先の動作までを含んで回避動作を考えるという方法が、有効ではないかと予想される。
しかし、この計算はロボットが動作するリアルタイムで行える必要があるため、計算量と時間の問題が発生すると考えられる。

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=15cm, ]{avoid_pfc_kakukaku.pdf}
    \caption{Robot behavior by PFC method with avoidance}
    \label{fig:avoid pfc kakukaku}
  \end{center}
\end{figure}

最後に、通常のPFC法とQ-MDP法での動作について確認する。
まず、通常のPFC法による動作の様子を、図\ref{fig:pfc}に示す。
図の上段がタスク成功時の動作であり、下段がタスク失敗時の動作である。
通常のPFC方では、分布全体がかなりインコース側を走行し、多くのパーティクルが障害物内を移動しているのが分かる。
成功時と失敗時の違いは、パーティクルの平均姿勢$\bar{\V{x}}$を利用した行動と同様に、真の姿勢$\V{x}^{*}$がアウトコース側に存在しているか否かである。
成功時の右のコマでは、障害物を回避することに成功したのち、ゴールを探索する動作を行いタスク成功したことが分かる。

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=15cm, ]{pfc.pdf}
    \caption{Robot behavior by normal PFC method}
    \label{fig:pfc}
  \end{center}
\end{figure}

Q-MDP法の動作については、図\ref{fig:q-mdp}に示す。
上段のタスク成功時の動作と下段のタスク失敗時の動作ともに、いくつかのパーティクルは障害物内を移動しているものの、
大部分は障害物を回避するような動作を行っている。
障害物を回避後は、ゴールへと向かう。
成功時の動作では、分布の中央付近に$\V{x}^{*}$が存在しているため、ゴールへ入ることができている。
しかし、Q-MDP法はゴールの探索動作を行わないため、失敗時の動作ではゴール付近で行動が長時間停滞し失敗となる。

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=15cm, ]{q-mdp.pdf}
    \caption{Robot behavior by Q-MDP method}
    \label{fig:q-mdp}
  \end{center}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{考察} \label{section:考察}
